#!/usr/bin/env python3

import os
import re

def parse_file(file_path):
    with open(file_path, 'r') as file:
        content = file.read()

    # Extracting metadata
    metadata = {}
    metadata_patterns = {
        'Title': r'Title:\s*(.*)',
        'Expires': r'Expires:\s*(.*)',
        'Homepage': r'Homepage:\s*(.*)',
        'Help': r'Help:\s*(.*)',
        'License': r'License:\s*(.*)',
        'Total number of network filters': r'Total number of network filters:\s*(\d+)'
    }

    for key, pattern in metadata_patterns.items():
        match = re.search(pattern, content)
        if match:
            metadata[key] = match.group(1)

    # Extracting filters
    filters = re.findall(r'(\d+\.\d+\.\d+\.\d+\s+\S+)', content)

    return metadata, filters

def read_files(directory):
    files_data = []
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            file_path = os.path.join(directory, filename)
            metadata, filters = parse_file(file_path)
            files_data.append((metadata, filters))
    return files_data

directory = './lists'
files_data = read_files(directory)


import torch
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.functional import to_map_style_dataset

# URL tokenizer
url_pattern = re.compile(r'\b(?:https?://|www\.)\S+\b')
def url_tokenizer(text):
    return url_pattern.findall(text)

# Build vocabulary
def yield_tokens(data_iter):
    for _, filters in data_iter:
        for filter in filters:
            yield url_tokenizer(filter)

vocab = build_vocab_from_iterator(yield_tokens(files_data), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

# Text pipeline
def text_pipeline(text):
    return vocab(url_tokenizer(text))

# Label pipeline
def label_pipeline(label):
    return 1 if label == 'some_label' else 0  # Replace 'some_label' with actual label

# Create dataset
def create_dataset(files_data):
    texts = []
    labels = []
    for metadata, filters in files_data:
        text = ' '.join(filters)
        texts.append(text)
        labels.append(metadata['Title'])  # Replace with actual label extraction
    return texts, labels

texts, labels = create_dataset(files_data)

# Convert to map-style dataset
text_pipeline = lambda x: torch.tensor([text_pipeline(x)])
label_pipeline = lambda x: torch.tensor(label_pipeline(x))

texts = [text_pipeline(text) for text in texts]
labels = [label_pipeline(label) for label in labels]

dataset = list(zip(texts, labels))
dataset = to_map_style_dataset(dataset)

import torch.nn as nn
import torch.optim as optim

class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.fc = nn.Linear(embed_dim, num_class)

    def forward(self, text):
        embedded = self.embedding(text).mean(dim=1)
        return self.fc(embedded)

# Hyperparameters
vocab_size = len(vocab)
embed_dim = 128
num_class = 2  # Replace with the actual number of classes
model = TextClassificationModel(vocab_size, embed_dim, num_class)

from torch.utils.data import DataLoader, random_split

# Split dataset
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for texts, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

# Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for texts, labels in test_loader:
        outputs = model(texts)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy: {100 * correct / total}%')
