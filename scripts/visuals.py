#!/usr/bin/env python3
"""
OpenSnitch Smart Policy Log Analyzer

This script analyzes logs generated by the OpenSnitch Smart Policy with
Reinforcement Learning to produce publication-quality visualizations.
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
from datetime import datetime, timedelta
import argparse
from collections import defaultdict

# Configure plot style for publication
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['legend.fontsize'] = 12
plt.rcParams['figure.titlesize'] = 18

# Define action mapping
ACTION_MAP = {
    0: "ALLOW_ONCE",
    1: "ALLOW_TEMP",
    2: "ALLOW_PERM",
    3: "BLOCK_ONCE",
    4: "BLOCK_TEMP",
    5: "BLOCK_PERM",
    6: "ASK_USER"
}

def parse_log_file(log_path):
    """
    Parse OpenSnitch log file into a pandas DataFrame
    """
    data = []
    
    # Regular expressions for extracting information
    timestamp_pattern = r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})"
    connection_pattern = r"Connection (\d+)/(\d+): (.+) → (.+?):(\d+) \| Action: (.+)"
    reward_pattern = r"reward: ([-\d\.]+)"
    reputation_pattern = r"reputation: ([-\d\.]+)"
    
    with open(log_path, 'r') as file:
        for line in file:
            # Extract timestamp
            timestamp_match = re.search(timestamp_pattern, line)
            if not timestamp_match:
                continue
                
            timestamp = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S,%f')
            
            # Extract connection info
            conn_match = re.search(connection_pattern, line)
            if conn_match:
                conn_num = int(conn_match.group(1))
                total_conns = int(conn_match.group(2))
                app = conn_match.group(3)
                domain = conn_match.group(4)
                port = int(conn_match.group(5))
                action = conn_match.group(6)
                
                # Extract reward if available
                reward = None
                reward_match = re.search(reward_pattern, line)
                if reward_match:
                    reward = float(reward_match.group(1))
                    
                # Extract reputation if available
                reputation = None
                reputation_match = re.search(reputation_pattern, line)
                if reputation_match:
                    reputation = float(reputation_match.group(1))
                
                data.append({
                    'timestamp': timestamp,
                    'conn_num': conn_num,
                    'total_conns': total_conns,
                    'app': app,
                    'domain': domain,
                    'port': port,
                    'action': action,
                    'reward': reward,
                    'reputation': reputation
                })
    
    # Create DataFrame
    df = pd.DataFrame(data)
    if not df.empty:
        # Convert action strings to categories
        df['action_category'] = df['action'].apply(lambda x: next((k for k, v in ACTION_MAP.items() if v == x), -1))
        
        # Add binary columns for allowing/blocking
        df['is_allow'] = df['action'].str.startswith('ALLOW')
        df['is_block'] = df['action'].str.startswith('BLOCK')
        df['is_ask'] = df['action'].str.startswith('ASK')
        
        # Add relative time in minutes from first log entry
        if 'timestamp' in df.columns and not df.empty:
            df['time_minutes'] = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60
    
    return df

def plot_policy_evolution(df, output_path='policy_evolution.png'):
    """
    Plot the evolution of decision policy over time, showing how the distribution
    of actions changes as the agent learns.
    """
    if df.empty:
        print("Warning: Empty dataframe, cannot create policy evolution plot")
        return
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Create time bins
    n_bins = min(50, len(df) // 10)  # Reasonable number of bins
    if n_bins < 5:  # If not enough data
        n_bins = 5
    
    # Calculate moving averages for each action type
    window_size = max(5, len(df) // 20)  # Dynamic window size
    
    # Group by time bins
    df['time_bin'] = pd.qcut(df['time_minutes'], n_bins, duplicates='drop')
    
    # Count actions by bin
    action_counts = df.groupby(['time_bin', 'action']).size().unstack(fill_value=0)
    
    # Convert to percentages
    action_pcts = action_counts.div(action_counts.sum(axis=1), axis=0) * 100
    
    # Plotting
    action_pcts.plot(kind='area', stacked=True, alpha=0.7, ax=ax, 
                    colormap='viridis')
    
    # Add smoothed decision boundary lines
    for action in action_pcts.columns:
        if action_pcts[action].mean() > 5:  # Only show major actions
            smoothed = action_pcts[action].rolling(window=3, min_periods=1).mean()
            ax.plot(range(len(smoothed)), smoothed, linewidth=2, label=f"{action} trend")
    
    # Calculate metrics for annotation
    final_window = action_pcts.iloc[-5:].mean()
    most_common_action = final_window.idxmax()
    most_common_pct = final_window.max()
    
    # Add annotations
    ax.set_title('Evolution of Decision Policy Over Time', fontweight='bold')
    ax.set_xlabel('Time Progression (Binned by Connection Sequence)')
    ax.set_ylabel('Percentage of Decisions (%)')
    
    # Add text annotation about the final policy
    ax.text(0.5, 0.01, 
            f"Final policy preference: {most_common_action} ({most_common_pct:.1f}%)",
            transform=ax.transAxes, ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))
    
    # Improve x-axis labels
    x_labels = [f"{bin.left:.1f}-{bin.right:.1f}" for bin in action_pcts.index]
    ax.set_xticks(range(len(x_labels)))
    ax.set_xticklabels([])  # First remove all labels
    
    # Then add labels only at positions 0, 25%, 50%, 75%, and 100%
    positions = [0, len(x_labels)//4, len(x_labels)//2, (3*len(x_labels))//4, len(x_labels)-1]
    labels = ["Start"] + [f"{pos//(len(x_labels)//4) * 25}%" for pos in positions[1:]]
    
    for pos, label in zip(positions, labels):
        ax.text(pos, -0.05, label, transform=ax.get_xaxis_transform(), ha='center')
    
    # Add grid
    ax.grid(True, alpha=0.3)
    
    # Adjust legend
    handles, labels = ax.get_legend_handles_labels()
    unique_labels = []
    unique_handles = []
    for handle, label in zip(handles, labels):
        if label not in unique_labels:
            unique_labels.append(label)
            unique_handles.append(handle)
    
    ax.legend(unique_handles, unique_labels, loc='upper center', 
              bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=True, title="Actions")
    
    # Adjust layout and save
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Policy evolution plot saved to {output_path}")

def plot_reward_analysis(df, output_path='reward_analysis.png'):
    """
    Plot the reward trends over time to show learning progress and performance.
    """
    if df.empty or 'reward' not in df.columns or df['reward'].isna().all():
        print("Warning: No reward data available for plotting")
        return
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, 
                                   gridspec_kw={'height_ratios': [2, 1]})
    
    # Fill NA rewards with 0
    df_plot = df.copy()
    df_plot['reward'] = df_plot['reward'].fillna(0)
    
    # Calculate rolling averages
    window_size = max(10, len(df_plot) // 50)
    df_plot['reward_rolling_avg'] = df_plot['reward'].rolling(window=window_size, min_periods=1).mean()
    df_plot['reward_rolling_std'] = df_plot['reward'].rolling(window=window_size, min_periods=1).std()
    
    # Calculate exponential weighted average (more weight to recent rewards)
    df_plot['reward_ema'] = df_plot['reward'].ewm(span=window_size, adjust=False).mean()
    
    # Plot individual rewards as scatter
    ax1.scatter(df_plot['conn_num'], df_plot['reward'], alpha=0.3, s=15, c='gray', label='Individual Rewards')
    
    # Plot moving average
    ax1.plot(df_plot['conn_num'], df_plot['reward_rolling_avg'], 
            linewidth=2.5, color='blue', label=f'Moving Average (window={window_size})')
    
    # Plot EMA
    ax1.plot(df_plot['conn_num'], df_plot['reward_ema'], 
            linewidth=2, color='green', label='Exponential Moving Average')
    
    # Plot standard deviation band
    ax1.fill_between(df_plot['conn_num'],
                    df_plot['reward_rolling_avg'] - df_plot['reward_rolling_std'],
                    df_plot['reward_rolling_avg'] + df_plot['reward_rolling_std'],
                    alpha=0.2, color='blue', label='±1σ')
    
    # Calculate and plot cumulative reward
    df_plot['cumulative_reward'] = df_plot['reward'].cumsum()
    ax2.plot(df_plot['conn_num'], df_plot['cumulative_reward'], 
             color='purple', linewidth=2, label='Cumulative Reward')
    
    # Calculate learning phases
    # Phase detection based on derivative of EMA
    df_plot['ema_diff'] = df_plot['reward_ema'].diff().fillna(0)
    
    # Detect significant changes in the learning trend
    threshold = df_plot['ema_diff'].std() * 1.5
    change_points = df_plot.index[abs(df_plot['ema_diff']) > threshold].tolist()
    
    # Group close change points
    if change_points:
        grouped_points = []
        current_group = [change_points[0]]
        
        for i in range(1, len(change_points)):
            if change_points[i] - current_group[-1] <= window_size:
                current_group.append(change_points[i])
            else:
                grouped_points.append(current_group[0])
                current_group = [change_points[i]]
        
        if current_group:
            grouped_points.append(current_group[0])
        
        # Add phase transition lines
        phase_names = ["Initial", "Learning", "Refining", "Converging", "Optimized"]
        phase_idx = 0
        
        for point in grouped_points[:4]:  # Limit to 4 transitions (5 phases)
            if phase_idx < len(phase_names) - 1:
                x_pos = df_plot.iloc[point]['conn_num']
                ax1.axvline(x=x_pos, color='red', linestyle='--', alpha=0.7)
                ax2.axvline(x=x_pos, color='red', linestyle='--', alpha=0.7)
                
                # Add phase labels
                mid_x = (prev_x + x_pos) / 2 if phase_idx > 0 else x_pos / 2
                ax1.text(mid_x, ax1.get_ylim()[1] * 0.9, phase_names[phase_idx], 
                        ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))
                
                prev_x = x_pos
                phase_idx += 1
        
        # Add final phase label
        if phase_idx < len(phase_names):
            mid_x = (prev_x + df_plot['conn_num'].max()) / 2
            ax1.text(mid_x, ax1.get_ylim()[1] * 0.9, phase_names[phase_idx], 
                    ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))
    
    # Calculate convergence metrics
    last_quarter = df_plot.iloc[int(len(df_plot)*0.75):]
    converged_reward = last_quarter['reward_rolling_avg'].mean()
    reward_stability = last_quarter['reward'].std()
    
    # Add annotations
    ax1.set_title('Reward Trends During Learning Process', fontweight='bold')
    ax1.set_ylabel('Reward Value')
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # Add text about convergence
    ax1.text(0.02, 0.02, 
             f"Converged reward: {converged_reward:.2f}\nReward stability: {reward_stability:.2f}",
             transform=ax1.transAxes, ha='left', va='bottom', fontsize=10,
             bbox=dict(facecolor='white', alpha=0.7))
    
    ax2.set_title('Cumulative Reward')
    ax2.set_xlabel('Connection Number')
    ax2.set_ylabel('Cumulative Reward')
    ax2.legend(loc='upper left')
    ax2.grid(True, alpha=0.3)
    
    # Add final performance annotation
    final_reward = df_plot['cumulative_reward'].iloc[-1]
    ax2.text(0.98, 0.05, f"Final cumulative reward: {final_reward:.2f}",
             transform=ax2.transAxes, ha='right', va='bottom', fontsize=10,
             bbox=dict(facecolor='white', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Reward analysis plot saved to {output_path}")

def plot_connection_heatmap(df, output_path='connection_heatmap.png'):
    """
    Plot a heatmap showing the connection patterns between applications and destinations,
    colored by decision type.
    """
    if df.empty:
        print("Warning: Empty dataframe, cannot create connection heatmap")
        return
    
    # Extract top applications and domains by connection count
    top_apps_count = min(10, df['app'].nunique())
    top_domains_count = min(15, df['domain'].nunique())
    
    top_apps = df['app'].value_counts().nlargest(top_apps_count).index.tolist()
    top_domains = df['domain'].value_counts().nlargest(top_domains_count).index.tolist()
    
    # Filter dataframe to top apps and domains
    df_top = df[df['app'].isin(top_apps) & df['domain'].isin(top_domains)]
    
    # Create pivot table: applications x domains with action counts
    pivot_allow = pd.pivot_table(
        df_top[df_top['is_allow']], 
        values='conn_num', 
        index='app', 
        columns='domain', 
        aggfunc='count',
        fill_value=0
    )
    
    pivot_block = pd.pivot_table(
        df_top[df_top['is_block']], 
        values='conn_num', 
        index='app', 
        columns='domain', 
        aggfunc='count',
        fill_value=0
    )
    
    pivot_ask = pd.pivot_table(
        df_top[df_top['is_ask']], 
        values='conn_num', 
        index='app', 
        columns='domain', 
        aggfunc='count',
        fill_value=0
    )
    
    # Create a combined matrix for plotting
    matrix_shape = (len(top_apps), len(top_domains))
    matrix = np.zeros(matrix_shape + (3,))
    
    # Fill in the RGB channels with normalized values for Allow (G), Block (R), Ask (B)
    for i, app in enumerate(top_apps):
        for j, domain in enumerate(top_domains):
            allow_val = pivot_allow.loc[app, domain] if app in pivot_allow.index and domain in pivot_allow.columns else 0
            block_val = pivot_block.loc[app, domain] if app in pivot_block.index and domain in pivot_block.columns else 0
            ask_val = pivot_ask.loc[app, domain] if app in pivot_ask.index and domain in pivot_ask.columns else 0
            
            total = allow_val + block_val + ask_val
            if total > 0:
                matrix[i, j, 0] = block_val / total  # Red channel (Block)
                matrix[i, j, 1] = allow_val / total  # Green channel (Allow)
                matrix[i, j, 2] = ask_val / total    # Blue channel (Ask)
    
    # Create figure
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # Plot heatmap
    im = ax.imshow(matrix)
    
    # Set ticks
    ax.set_xticks(np.arange(len(top_domains)))
    ax.set_yticks(np.arange(len(top_apps)))
    
    # Set tick labels
    ax.set_xticklabels([d[:20] + '...' if len(d) > 20 else d for d in top_domains], rotation=45, ha='right')
    ax.set_yticklabels([a[:25] + '...' if len(a) > 25 else a for a in top_apps])
    
    # Add counts as text
    for i, app in enumerate(top_apps):
        for j, domain in enumerate(top_domains):
            allow_val = pivot_allow.loc[app, domain] if app in pivot_allow.index and domain in pivot_allow.columns else 0
            block_val = pivot_block.loc[app, domain] if app in pivot_block.index and domain in pivot_block.columns else 0
            ask_val = pivot_ask.loc[app, domain] if app in pivot_ask.index and domain in pivot_ask.columns else 0
            
            if allow_val + block_val + ask_val > 0:
                # Determine text color based on background brightness
                text_color = 'white' if np.mean(matrix[i, j]) < 0.5 else 'black'
                
                text = f"A:{allow_val}\nB:{block_val}"
                if ask_val > 0:
                    text += f"\nU:{ask_val}"
                
                ax.text(j, i, text, ha='center', va='center', color=text_color, fontsize=9)
    
    # Add title and labels
    ax.set_title('Connection Patterns and Decisions by Application and Destination', fontweight='bold')
    ax.set_xlabel('Destination Domains')
    ax.set_ylabel('Applications')
    
    # Add RGB color legend
    # Create a custom colorbar with examples
    legend_elements = [
        plt.Rectangle((0, 0), 1, 1, facecolor=(0, 1, 0), label='Allow'),
        plt.Rectangle((0, 0), 1, 1, facecolor=(1, 0, 0), label='Block'),
        plt.Rectangle((0, 0), 1, 1, facecolor=(0, 0, 1), label='Ask User'),
        plt.Rectangle((0, 0), 1, 1, facecolor=(0.5, 0.5, 0), label='Mixed Allow/Block'),
        plt.Rectangle((0, 0), 1, 1, facecolor=(0.33, 0.33, 0.33), label='Equal Mix')
    ]
    
    ax.legend(handles=legend_elements, loc='upper center', 
              bbox_to_anchor=(0.5, -0.1), ncol=5)
    
    # Add annotation about interpretation
    plt.figtext(0.5, 0.01, 
                "Color indicates decision distribution: Green=Allow, Red=Block, Blue=Ask User\n" + 
                "Cell values show counts for: A=Allow, B=Block, U=Ask User",
                ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Connection heatmap saved to {output_path}")

def main():
    """Main function to process logs and generate visualizations"""
    parser = argparse.ArgumentParser(description='Analyze OpenSnitch Smart Policy logs and generate visualizations')
    parser.add_argument('--log', required=True, help='Path to the agent log file')
    parser.add_argument('--output-dir', default='visualizations', help='Output directory for visualizations')
    args = parser.parse_args()
    
    # Ensure output directory exists
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"Analyzing log file: {args.log}")
    
    # Parse log file
    df = parse_log_file(args.log)
    
    if df.empty:
        print("Error: No valid data found in log file")
        return
    
    print(f"Processed {len(df)} log entries")
    
    # Generate visualizations
    plot_policy_evolution(df, os.path.join(args.output_dir, 'policy_evolution.png'))
    plot_reward_analysis(df, os.path.join(args.output_dir, 'reward_analysis.png'))
    plot_connection_heatmap(df, os.path.join(args.output_dir, 'connection_heatmap.png'))
    
    print("Analysis complete. Visualizations saved to:", args.output_dir)

if __name__ == "__main__":
    main()
