#!/usr/bin/env python3

import argparse
from os.path import isdir, isfile
from nltk.tokenize import word_tokenize
  
parser = argparse.ArgumentParser(description="A simple CLI tool for tokenizing text")

parser.add_argument("-d", help="process all text files in that dir", type=str)
parser.add_argument("-f", help="mark the input as a dir", type=str)

import os
import re

def parse_file(file_path):
    with open(file_path, 'r') as file:
        content = file.read()

    # Extracting metadata
    metadata = {}
    metadata_patterns = {
        'Title': r'Title:\s*(.*)',
        'Expires': r'Expires:\s*(.*)',
        'Homepage': r'Homepage:\s*(.*)',
        'Help': r'Help:\s*(.*)',
        'License': r'License:\s*(.*)',
        'Total number of network filters': r'Total number of network filters:\s*(\d+)'
    }

    for key, pattern in metadata_patterns.items():
        match = re.search(pattern, content)
        if match:
            metadata[key] = match.group(1)

    # Extracting filters
    filters = re.findall(r'(\d+\.\d+\.\d+\.\d+\s+\S+)', content)

    return metadata, filters

def read_files(directory):
    files_data = []
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            file_path = os.path.join(directory, filename)
            metadata, filters = parse_file(file_path)
            files_data.append((metadata, filters))
    return files_data

if __name__ == "__main__":
  args = parser.parse_args()

  if args.d is not None:
    files_data = read_files(args.d)

